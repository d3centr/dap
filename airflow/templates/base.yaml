apiVersion: batch/v1
kind: Job
metadata:
  name: {{ .Release.Name }}-base-build
  annotations:
    argocd.argoproj.io/hook: {{ .Values.baseBuild }}
    helm.sh/hook-weight: "-2"
    helm.sh/hook-delete-policy: before-hook-creation
spec:
  template:
    metadata:
      name: {{ .Release.Name }}-base-build
    spec:
      restartPolicy: Never
      containers:
      - name: {{ .Release.Name }}-base-build
        image: gcr.io/kaniko-project/executor:{{ .Values.kanikoVersion }}
        args:
        - "--context=/mnt"
        - "--destination={{ .Values.airflow.defaultAirflowRepository }}:base-{{ .Values.airflow.airflowVersion }}"
        - "--cache=false"
        env:
        volumeMounts:
        - name: dockerfile
          mountPath: /mnt
        - name: dap
          mountPath: /.dap/cluster
        {{ range .Values.dapps }}
        - name: {{ .dapp }}
          mountPath: /.dap/pipeline 
        {{ end }}
      volumes:
      - name: dockerfile
        configMap:
          name: {{ .Release.Name }}-base-dockerfile
      - name: dap
        secret:
          secretName: dap-deploy-key
          optional: true
          defaultMode: 0444
      {{ range .Values.dapps }}
      - name: {{ .dapp }}
        secret:
          secretName: {{ .dapp }}-deploy-key
          optional: true
          defaultMode: 0444
          items:
          - key: key
            path: {{ .dapp }}
      {{ end }}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Release.Name }}-base-dockerfile
  annotations:
    argocd.argoproj.io/hook: {{ .Values.baseBuild }}
    helm.sh/hook-weight: "-2"
    helm.sh/hook-delete-policy: before-hook-creation
data:
  Dockerfile: |
    FROM apache/airflow:{{ .Values.airflow.airflowVersion }}

    USER root
    # g++ for cc1plus dependency in sasl (hive provider requirement)
    RUN apt-get update && \
        apt-get install -y git gcc libsasl2-dev g++ && \
        rm -rf /var/lib/apt/lists/*

    USER airflow
    RUN pip install {{ join " " .Values.pipPackages }}

    # - 'dap' added at the end of git clone command to support arbitrary repo names
    # - mkdir dags/test ensures that content is linked into 'test/dap' rather than 'test'
    RUN GIT_SSH_COMMAND='ssh -o StrictHostKeyChecking=no -i /.dap/cluster/key' \
        git clone --single-branch --depth 1 \
            --branch {{ .Values.airflow.dags.gitSync.branch }} \
            {{ .Values.airflow.dags.gitSync.repo }} dap && \
        ln -s $PWD/dap/{{ .Values.airflow.dags.gitSync.subPath }}/dap dags && \
        mkdir dags/test && \
        ln -s $PWD/dap/{{ .Values.airflow.dags.gitSync.subPath }}/test/dap dags/test

    {{ range .Values.dapps }}
      {{ if has "airflow" .apps }}

    RUN GIT_SSH_COMMAND='ssh -o StrictHostKeyChecking=no -i /.dap/pipeline/{{ .dapp }}' \
        git clone --single-branch --depth 1 \
            --branch {{ .branch }} {{ .repo }} {{ .dapp }} && \
        mkdir -p dags/dapp && \
        ln -s $PWD/{{ .dapp }}/{{ .path }}/airflow/dags/dapp/{{ .dapp }} dags/dapp && \
        mkdir -p dags/test/dapp && \
        ln -s $PWD/{{ .dapp }}/{{ .path }}/airflow/dags/test/dapp/{{ .dapp }} dags/test/dapp

      {{ end }}
    {{ end }}

    RUN echo "AUTH_ROLE_PUBLIC = 'Admin'" >> /opt/airflow/webserver_config.py
    ENV AWS_DEFAULT_REGION {{ .Values.region }}
    ENV AIRFLOW_CONN_HIVESERVER2_DEFAULT hiveserver2://sparkubi-thrift.spark.svc.cluster.local:10000
    ENV AIRFLOW__CORE__PARALLELISM 64
    ENV AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG 32
    ENV AIRFLOW__API__AUTH_BACKEND airflow.api.auth.backend.basic_auth
    # below available from Airflow 2.3: note plural variable change
    # ENV AIRFLOW__API__AUTH_BACKENDS airflow.api.auth.backend.session

