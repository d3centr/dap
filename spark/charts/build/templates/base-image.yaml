{{ $profile := tpl ".Values.global.profiles.{{ .Values.global.profile }}" . }}
{{ $baseBuild := tpl (printf "{{ %s.baseBuild }}" $profile) . }}
apiVersion: batch/v1
kind: Job
metadata:
  name: {{ .Release.Name }}-dist-image
  annotations:
    argocd.argoproj.io/hook: {{ $baseBuild }}
    helm.sh/hook-weight: "-3"
    helm.sh/hook-delete-policy: before-hook-creation
spec:
  template:
    metadata:
      name: {{ .Release.Name }}-dist-image
    spec:
      restartPolicy: Never
      containers:
      - name: base-image
        image: gcr.io/kaniko-project/executor:{{ .Values.global.kanikoVersion }}
        args:
        - "--context=/mnt"
        - "--destination={{ .Values.global.registry }}:{{ .Release.Name }}-dist-{{ .Values.sparkVersion }}"
        - "--cache=true"
        resources:
          requests:
            cpu: 1800m
            memory: 7Gi
        volumeMounts:
        - name: dockerfile
          mountPath: /mnt
      volumes:
      - name: dockerfile
        configMap:
          name: {{ .Release.Name }}-dist-dockerfile
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Release.Name }}-dist-dockerfile
  annotations:
    argocd.argoproj.io/hook: {{ $baseBuild }}
    helm.sh/hook-weight: "-3"
    helm.sh/hook-delete-policy: before-hook-creation
data:
  Dockerfile: |
    # build Spark distribution from source
    FROM openjdk:11-jdk-slim AS dist
    ARG path=/tmp/spark-{{ .Values.sparkVersion }}

    RUN apt-get update && apt-get install -y curl && rm -rf /var/cache/apt/*

    RUN curl -L -o $path.tar.gz https://github.com/apache/spark/archive/v{{ .Values.sparkVersion }}.tar.gz && \
        tar xzvf $path.tar.gz -C /tmp

    RUN export MAVEN_OPTS="-Xss64m -Xmx2g -XX:ReservedCodeCacheSize=1g" && \
        $path/dev/make-distribution.sh --name dap-dist \
            -Phive \
            -Phive-thriftserver \
            -Pkubernetes
---
apiVersion: batch/v1
kind: Job
metadata:
  name: {{ .Release.Name }}-base-image
  annotations:
    argocd.argoproj.io/hook: {{ $baseBuild }}
    helm.sh/hook-weight: "-2"
    helm.sh/hook-delete-policy: before-hook-creation
spec:
  template:
    metadata:
      name: {{ .Release.Name }}-base-image
    spec:
      restartPolicy: Never
      containers:
      - name: base-image
        image: gcr.io/kaniko-project/executor:{{ .Values.global.kanikoVersion }}
        args:
        - "--context=/mnt"
        - "--destination={{ .Values.global.registry }}:{{ .Release.Name }}-base-{{ .Values.sparkVersion }}"
        - "--cache=true"
        resources:
          requests:
            cpu: 1800m
            memory: 7Gi
        volumeMounts:
        - name: dockerfile
          mountPath: /mnt
      volumes:
      - name: dockerfile
        configMap:
          name: {{ .Release.Name }}-base-dockerfile
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Release.Name }}-base-dockerfile
  annotations:
    argocd.argoproj.io/hook: {{ $baseBuild }}
    helm.sh/hook-weight: "-2"
    helm.sh/hook-delete-policy: before-hook-creation
data:
  Dockerfile: |
    FROM {{ .Values.global.registry }}:{{ .Release.Name }}-dist-{{ .Values.sparkVersion }} AS dist

    # base Spark image referencing artefacts built above
    FROM openjdk:11-jre-slim
    ARG path=/tmp/spark-{{ .Values.sparkVersion }}/dist

    RUN set -ex && \
        sed -i 's/http:\/\/deb.\(.*\)/https:\/\/deb.\1/g' /etc/apt/sources.list && \
        apt-get update && \
        ln -s /lib /lib64 && \
        apt install -y bash tini libc6 libpam-modules krb5-user libnss3 procps && \
        mkdir -p /opt/spark && \
        mkdir -p /opt/spark/examples && \
        mkdir -p /opt/spark/work-dir && \
        touch /opt/spark/RELEASE && \
        rm /bin/sh && \
        ln -sv /bin/bash /bin/sh && \
        echo "auth required pam_wheel.so use_uid" >> /etc/pam.d/su && \
        chgrp root /etc/passwd && chmod ug+rw /etc/passwd && \
        rm -rf /var/cache/apt/*

    COPY --from=dist $path/jars /opt/spark/jars
    COPY --from=dist $path/bin /opt/spark/bin
    COPY --from=dist $path/sbin /opt/spark/sbin
    COPY --from=dist $path/kubernetes/dockerfiles/spark/entrypoint.sh /opt/
    COPY --from=dist $path/kubernetes/dockerfiles/spark/decom.sh /opt/
    COPY --from=dist $path/kubernetes/tests /opt/spark/tests

    ENV SPARK_HOME /opt/spark

    WORKDIR /opt/spark/work-dir
    RUN chmod g+w /opt/spark/work-dir
    RUN chmod a+x /opt/decom.sh

    ENTRYPOINT [ "/opt/entrypoint.sh" ]

    USER 185

