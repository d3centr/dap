apiVersion: batch/v1
kind: Job
metadata:
  name: {{ .Release.Name }}-builder
  annotations:
    argocd.argoproj.io/hook: {{ .Values.global.coldBuild }}
    helm.sh/hook-weight: "1"
    helm.sh/hook-delete-policy: before-hook-creation
spec:
  template:
    metadata:
      name: {{ .Release.Name }}-builder
    spec:
      restartPolicy: Never
      containers:
      - name: {{ .Release.Name }}-builder
        image: gcr.io/kaniko-project/executor:latest
        args:
        - "--context=/mnt"
        - "--destination={{ .Values.global.registry }}:{{ .Release.Name }}-builder-{{ .Values.global.appVersion }}"
        - "--cache=false"
        - "--build-arg=KUBECTL_VERSION"
        # pass KUBECTL_VERSION from global variables
        envFrom:
        - configMapRef:
            name: env
        resources:
          requests:
            cpu: 1800m
            memory: 2900Mi
        volumeMounts:
        - name: dockerfile
          mountPath: /mnt
      volumes:
      - name: dockerfile
        configMap:
          name: {{ .Release.Name }}-cold-dockerfile
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Release.Name }}-cold-dockerfile
  annotations:
    argocd.argoproj.io/hook: {{ .Values.global.coldBuild }}
    helm.sh/hook-weight: "1"
    helm.sh/hook-delete-policy: before-hook-creation
data:
  Dockerfile: |
    FROM {{ .Values.global.registry }}:{{ tpl .Values.baseImage . }}

    USER root

    # spark-submit relies on kubectl rather than specialized wrappers around K8s API
    ARG KUBECTL_VERSION
    RUN apt-get update && apt-get install -y git curl unzip zip && rm -rf /var/cache/apt/* && \
        curl -s "https://get.sdkman.io" | bash && \
        curl -LO https://dl.k8s.io/release/v$KUBECTL_VERSION/bin/linux/amd64/kubectl && \
        install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl && rm kubectl

    RUN git clone --single-branch --depth 1 --branch {{ .Values.branch }} {{ .Values.repo }}

    ARG MVN=https://repo1.maven.org/maven2  # referenced in extraJars, e.g. uniswap/values.yaml
    RUN source /root/.sdkman/bin/sdkman-init.sh && cd dap/spark/{{ .Release.Name }} && \
        sdk install sbt `awk '$1=="sbtVersion" && $2==":=" {print $3}' build.sbt | tr -d '"'` && \
        ln -s /opt/spark/jars lib && \
    {{ range .Values.extraCommands }}
        {{ . }} && \
    {{ end }}    
        mkdir -p target/scala-{{ .Values.scalaVersion }} && \
        cd target/scala-{{ .Values.scalaVersion }} && \
    {{ if and (hasPrefix "sparkubi" .Values.baseImage) (ne .Release.Name "sparkubi") }}
        cp /opt/spark/jars/sparkubi*.jar . && \
    {{ end }}
    {{ range .Values.extraJars }}
        curl -O {{ . }} && \
    {{ end }}
        cd - && \
        sbt package

    WORKDIR /opt/spark/work-dir/dap/spark
---
apiVersion: batch/v1
kind: Job
metadata:
  name: {{ .Release.Name }}-build
  annotations:
    argocd.argoproj.io/hook: {{ .Values.global.hotBuild }}
    helm.sh/hook-weight: "2"
    helm.sh/hook-delete-policy: before-hook-creation
spec:
  template:
    metadata:
      name: {{ .Release.Name }}-build
    spec:
      restartPolicy: Never
      containers:
      - name: {{ .Release.Name }}-build
        image: gcr.io/kaniko-project/executor:latest
        args:
        - "--context=/mnt"
        - "--destination={{ .Values.global.registry }}:{{ .Release.Name }}-{{ .Values.global.appVersion }}"
        - "--cache=false"
        resources:
          requests:
            cpu: 1200m
            memory: 1800Mi
        volumeMounts:
        - name: dockerfile
          mountPath: /mnt
      volumes:
      - name: dockerfile
        configMap:
          name: {{ .Release.Name }}-hot-dockerfile
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Release.Name }}-hot-dockerfile
  annotations:
    argocd.argoproj.io/hook: {{ .Values.global.hotBuild }}
    helm.sh/hook-weight: "2"
    helm.sh/hook-delete-policy: before-hook-creation
data:
  Dockerfile: |
    FROM {{ .Values.global.registry }}:{{ .Release.Name }}-builder-{{ .Values.global.appVersion }} AS build

    RUN git pull && \
        source /root/.sdkman/bin/sdkman-init.sh && cd {{ .Release.Name }} && sbt package

    FROM {{ .Values.global.registry }}:{{ .Release.Name }}-dep-{{ .Values.sparkVersion }}
    COPY --from=build /opt/spark/work-dir/dap/spark/{{ .Release.Name }}/target/scala-{{ .Values.scalaVersion }}/*.jar /opt/spark/jars/

